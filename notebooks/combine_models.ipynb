{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     # accepts commandline arguments\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='Process train data and test data.')\n",
    "\n",
    "#     # Add arguments\n",
    "#     parser.add_argument('train_data', type=int, help='train dataset')\n",
    "#     parser.add_argument('test_data', type=int, help='test dataset')\n",
    "\n",
    "#     # Parse the arguments\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # Now you can use args.grid_id and args.n_components in your script\n",
    "#     train_data = args.train_data\n",
    "#     test_data = args.test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid_id = 333519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yinyue/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yinyue/Documents/Graduate/ENGI4800 Capstone/broken-banner-detection/notebooks/pca_pipeline.py\", line 6, in <module>\n",
      "    import seaborn as sns\n",
      "  File \"/Users/yinyue/opt/anaconda3/lib/python3.9/site-packages/seaborn/__init__.py\", line 12, in <module>\n",
      "    from .widgets import *  # noqa: F401,F403\n",
      "  File \"/Users/yinyue/opt/anaconda3/lib/python3.9/site-packages/seaborn/widgets.py\", line 7, in <module>\n",
      "    from ipywidgets import interact, FloatSlider, IntSlider\n",
      "  File \"/Users/yinyue/opt/anaconda3/lib/python3.9/site-packages/ipywidgets/__init__.py\", line 25, in <module>\n",
      "    from .widgets import *\n",
      "  File \"/Users/yinyue/opt/anaconda3/lib/python3.9/site-packages/ipywidgets/widgets/__init__.py\", line 5, in <module>\n",
      "    from .domwidget import DOMWidget\n",
      "  File \"/Users/yinyue/opt/anaconda3/lib/python3.9/site-packages/ipywidgets/widgets/domwidget.py\", line 8, in <module>\n",
      "    from .trait_types import InstanceDict, TypedTuple\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 846, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 941, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1040, in get_data\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 pca_pipeline.py 333519 200 '../datasets/train/data_heatmap_train.csv'\n",
    "\n",
    "pca_df = pd.read_csv('temp/pca_df.csv')\n",
    "pca_df = pca_df.iloc[:,1:]\n",
    "pca_df_inp = pca_df.iloc[:,:-2]\n",
    "y_train = pca_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yinyue/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Grid ID:  333519\n",
      "Nb components:  200\n",
      "Data directory:  ../datasets/test/data_heatmap_test.csv\n"
     ]
    }
   ],
   "source": [
    "!python3 pca_pipeline.py 333519 200 '../datasets/test/data_heatmap_test.csv'\n",
    "\n",
    "pca_test = pd.read_csv('temp/pca_df.csv')\n",
    "pca_test = pca_test.iloc[:,1:]\n",
    "y_test = pca_test[['label']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[557,  13],\n",
       "       [ 22,  18]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(pca_df_inp)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42).fit(data_normalized)\n",
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Calculate the distance from each point to its cluster centroid\n",
    "closest, distances = pairwise_distances_argmin_min(data_normalized, centroids[labels])\n",
    "# distances = euclidean_distances(data_normalized, centroids[labels])\n",
    "\n",
    "# Choose a threshold for flagging an anomaly\n",
    "threshold = np.percentile(distances, 95)\n",
    "\n",
    "# # Anything above the threshold is considered an anomaly\n",
    "anomalies = pca_df_inp[distances > threshold]\n",
    "\n",
    "kmeans_labels = (distances > threshold).astype(int)\n",
    "confusion_matrix(y_train,kmeans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best PCA dimension: 86\n",
      "Best accuracy: 0.99\n",
      "Best precision: 0.89\n",
      "Best recall: 0.97\n",
      "Best f1-score: 0.93\n"
     ]
    }
   ],
   "source": [
    "# kchoices = [1,2,3,4,5,6]\n",
    "\n",
    "best_acc = -float('inf')\n",
    "best_dim = -1\n",
    "best_recall = -1\n",
    "best_prec = -1\n",
    "best_f1 = -1\n",
    "\n",
    "# for k in kchoices:\n",
    "for pca_dim in np.linspace(2,200,50).astype(int):\n",
    "      X = pca_df_inp.iloc[:,:pca_dim]\n",
    "      kmeans = KMeans(n_clusters = 2, random_state=42)\n",
    "      kmeans_labels = kmeans.fit_predict(X)\n",
    "      # Identify the smallest cluster or the one furthest from others\n",
    "      # cluster_minority = pd.Series(clusters).value_counts()\n",
    "      # cluster_minority = cluster_minority[cluster_minority <= 60].index.get_level_values(0).tolist()\n",
    "      # kmeans_labels = np.isin(clusters, cluster_minority).astype(int)\n",
    "      conf_mat   = confusion_matrix(y_train,kmeans_labels)\n",
    "      acc        = accuracy_score(y_train,kmeans_labels)\n",
    "      recall     = recall_score(y_train,kmeans_labels)\n",
    "      prec       = precision_score(y_train,kmeans_labels)\n",
    "      f1         = f1_score(y_train,kmeans_labels)\n",
    "      \n",
    "      if f1 > best_f1:\n",
    "            best_acc = acc\n",
    "            best_dim = pca_dim\n",
    "            best_recall  = recall\n",
    "            best_prec = prec\n",
    "            best_f1 = f1\n",
    "            # best_k = k\n",
    "            # print(f'Confusion matrix for {pca_dim} PCA dimensions:\\n', \n",
    "            #       f' {conf_mat}')\n",
    "            # print(f'Accuracy for {pca_dim} PCA dimensions:', \n",
    "            #       f' {acc:.2f}')\n",
    "            # print(f'Recall for {pca_dim} PCA dimensions:', \n",
    "            #       f' {recall:.2f}')\n",
    "            # print(f'Precision for {pca_dim} PCA dimensions:', \n",
    "            #       f' {prec:.2f}')\n",
    "            # print(f'---------')\n",
    "#     print(f'-------------------------------------')\n",
    "# print(f'Best k: {best_k}')\n",
    "\n",
    "print(f'Best PCA dimension: {best_dim}')\n",
    "print(f'Best accuracy: {best_acc:.2f}')\n",
    "print(f'Best precision: {best_prec:.2f}')\n",
    "print(f'Best recall: {best_recall:.2f}')\n",
    "print(f'Best f1-score: {best_f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[241   4]\n",
      " [  2  15]]\n",
      "Test accuracy: 0.98\n",
      "Test precision: 0.79\n",
      "Test recall: 0.88\n",
      "Test f1-score: 0.83\n"
     ]
    }
   ],
   "source": [
    "X = pca_df_inp.iloc[:,:best_dim]\n",
    "kmeans = KMeans(n_clusters = 2, random_state=42)\n",
    "kmeans.fit(X)\n",
    "# clusters = pd.DataFrame(kmeans.labels_)\n",
    "# Identify the smallest cluster or the one furthest from others\n",
    "# cluster_minority = clusters.value_counts()\n",
    "# cluster_minority = cluster_minority[cluster_minority <= 60].index.get_level_values(0).tolist()\n",
    "kmeans_labels = kmeans.predict(pca_test.iloc[:,:best_dim])\n",
    "# kmeans_labels = np.isin(pred, cluster_minority).astype(int)\n",
    "conf_mat = confusion_matrix(y_test,kmeans_labels)\n",
    "acc = accuracy_score(y_test,kmeans_labels)\n",
    "recall = recall_score(y_test,kmeans_labels)\n",
    "prec = precision_score(y_test,kmeans_labels)\n",
    "f1 = f1_score(y_test,kmeans_labels)\n",
    "print(conf_mat)\n",
    "print(f'Test accuracy: {acc:.2f}')\n",
    "print(f'Test precision: {prec:.2f}')\n",
    "print(f'Test recall: {recall:.2f}')\n",
    "print(f'Test f1-score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix on X_test:\n",
      "[[233  12]\n",
      " [  2  15]]\n",
      "Accuracy on X_test: 0.95\n",
      "F1 Score on X_test: 0.68\n",
      "Predicted knn labels:\n",
      "[0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Perform PCA on X_train and X_test with 5 dimensions\n",
    "X_train = pca_df_inp.iloc[:, :5]  \n",
    "X_test = pca_test.iloc[:, :5] \n",
    "\n",
    "# Train k-NN model on X_train_pca with 10 neighbors\n",
    "knn = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "knn.fit(X_train)\n",
    "\n",
    "# Predict anomalies on X_test_pca\n",
    "distances, indices = knn.kneighbors(X_test)\n",
    "anomaly_scores = distances.mean(axis=1)\n",
    "threshold = np.percentile(anomaly_scores, 90)\n",
    "\n",
    "# Convert labels to binary (0: inliers, 1: outliers)\n",
    "knn_labels = np.where(anomaly_scores > threshold, 1, 0)\n",
    "\n",
    "# Evaluate the performance on X_test\n",
    "conf_mat = confusion_matrix(y_test, knn_labels)\n",
    "acc = accuracy_score(y_test, knn_labels)\n",
    "f1 = f1_score(y_test, knn_labels)\n",
    "\n",
    "print(f'Confusion matrix on X_test:\\n{conf_mat}')\n",
    "print(f'Accuracy on X_test: {acc:.2f}')\n",
    "print(f'F1 Score on X_test: {f1:.2f}')\n",
    "print(\"Predicted knn labels:\")\n",
    "print(knn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance on test data:\n",
      "Confusion matrix:\n",
      "[[235  10]\n",
      " [  0  17]]\n",
      "Accuracy: 0.9618320610687023\n",
      "F1 Score: 0.7727272727272727\n",
      "Predicted DBScan labels:\n",
      "[0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA with 8 dimensions\n",
    "X_train = pca_df_inp.iloc[:, :8]  \n",
    "X_test = pca_test.iloc[:, :8] \n",
    "\n",
    "# Fit DBSCAN model\n",
    "dbscan = DBSCAN(eps=10, min_samples=10)\n",
    "dbscan_labels_train = dbscan.fit_predict(X_train)\n",
    "dbscan_labels_test = dbscan.fit_predict(X_test)\n",
    "\n",
    "# Convert labels to binary (0: inliers, 1: outliers)\n",
    "dbscan_labels_train = np.where(dbscan_labels_train >= 0, 0, 1)\n",
    "dbscan_labels_test = np.where(dbscan_labels_test >= 0, 0, 1)\n",
    "\n",
    "# Evaluate performance on test data\n",
    "conf_mat_test = confusion_matrix(y_test, dbscan_labels_test)\n",
    "acc_test = accuracy_score(y_test, dbscan_labels_test)\n",
    "f1_test = f1_score(y_test, dbscan_labels_test)\n",
    "\n",
    "print(\"\\nPerformance on test data:\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_mat_test)\n",
    "print(\"Accuracy:\", acc_test)\n",
    "print(\"F1 Score:\", f1_test)\n",
    "print(\"Predicted DBScan labels:\")\n",
    "print(dbscan_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for 10 PCA dimensions:\n",
      "  [[224  21]\n",
      " [  1  16]]\n",
      "F1 Score for 10 PCA dimensions with n_estimator = 100:  0.59\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "#Best Model for Isolation Forest is with 10 PCA and n_estimators = 100\n",
    "best_n_est = 100\n",
    "pca_dim = 10\n",
    "X_train = pca_df_inp.iloc[:,:pca_dim]\n",
    "X_test = pca_test.iloc[:,:pca_dim]\n",
    "            \n",
    "isolation_forest = IsolationForest(n_estimators=best_n_est, \n",
    "                                   max_samples='auto', \n",
    "                                   random_state=42)\n",
    "\n",
    "model = isolation_forest.fit(X_train)\n",
    "if_labels = model.predict(X_test)\n",
    "if_labels = np.where(if_labels == -1, 1, 0)\n",
    "\n",
    "conf_mat   = confusion_matrix(y_test,if_labels)\n",
    "acc        = accuracy_score(y_test,if_labels)\n",
    "recall     = recall_score(y_test,if_labels)\n",
    "prec       = precision_score(y_test,if_labels)\n",
    "f1         = f1_score(y_test, if_labels)\n",
    "\n",
    "\n",
    "print(f'Confusion matrix for {pca_dim} PCA dimensions:\\n', \n",
    "                      f' {conf_mat}')\n",
    "#print(f'Accuracy for {pca_dim} PCA dimensions:', \n",
    "#                      f' {acc:.2f}')\n",
    "#print(f'Recall for {pca_dim} PCA dimensions:', \n",
    "#                      f' {recall:.2f}')\n",
    "#print(f'Precision for {pca_dim} PCA dimensions:', \n",
    "#                      f' {prec:.2f}')\n",
    "print(f'F1 Score for {pca_dim} PCA dimensions with n_estimator = {best_n_est}:', \n",
    "                      f' {f1:.2f}')\n",
    "print(f'---------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for 163 PCA dimensions:\n",
      "  [[ 57 188]\n",
      " [  0  17]]\n",
      "F1 Score for 163 PCA dimensions:  0.15\n"
     ]
    }
   ],
   "source": [
    "# Best Model is with 163 PCA dimensions\n",
    "pca_dim = 163\n",
    "\n",
    "X_train = pca_df_inp.iloc[:,:pca_dim]\n",
    "X_test = pca_test.iloc[:,:pca_dim]\n",
    "\n",
    "oneclass = OneClassSVM(gamma = 'auto').fit(X_train)\n",
    "oneclass_labels = oneclass.predict(X_test)\n",
    "    \n",
    "oneclass_labels = np.where(oneclass_labels == 1, 0,1)\n",
    "conf_mat   = confusion_matrix(y_test,oneclass_labels)\n",
    "acc        = accuracy_score(y_test,oneclass_labels)\n",
    "recall     = recall_score(y_test,oneclass_labels)\n",
    "prec       = precision_score(y_test,oneclass_labels)\n",
    "f1         = f1_score(y_test,oneclass_labels)\n",
    "\n",
    "print(f'Confusion matrix for {pca_dim} PCA dimensions:\\n', \n",
    "              f' {conf_mat}')\n",
    "#print(f'Accuracy for {pca_dim} PCA dimensions:', \n",
    "#              f' {acc:.2f}')\n",
    "#print(f'Recall for {pca_dim} PCA dimensions:', \n",
    "#              f' {recall:.2f}')\n",
    "#print(f'Precision for {pca_dim} PCA dimensions:', \n",
    "#              f' {prec:.2f}')\n",
    "print(f'F1 Score for {pca_dim} PCA dimensions:', \n",
    "              f' {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for ((1, 1, 1, 1, 1)) included and 1:\n",
      "  [[241   4]\n",
      " [  1  16]]\n",
      "Best f1 score: 0.87\n"
     ]
    }
   ],
   "source": [
    "best_acc = -float('inf')\n",
    "best_dim = -1\n",
    "best_recall = -1\n",
    "best_prec = -1\n",
    "best_f1 = -1\n",
    "\n",
    "for i1 in [0,1]:\n",
    "    for i2 in [0,1]:\n",
    "        for i3 in [0,1]:\n",
    "            for i4 in [0,1]:\n",
    "                for i5 in [0,1]:\n",
    "                    for thresh in [0,1]:\n",
    "                        tot = i1+i2+i3+i4+i5\n",
    "                        comb_labels = i1*if_labels + i2*oneclass_labels + i3*dbscan_labels_test + \\\n",
    "                                        i4*kmeans_labels + i5*knn_labels\n",
    "                        comb_labels = np.where(comb_labels > tot//2 + thresh, 1, 0)\n",
    "\n",
    "                        conf_mat   = confusion_matrix(y_test,comb_labels)\n",
    "                        acc        = accuracy_score(y_test,comb_labels)\n",
    "                        recall     = recall_score(y_test,comb_labels)\n",
    "                        prec       = precision_score(y_test,comb_labels)\n",
    "                        f1         = f1_score(y_test,comb_labels)\n",
    "\n",
    "                        if acc > best_acc:\n",
    "                            best_acc = acc\n",
    "                            best_recall  = recall\n",
    "                            best_prec = prec\n",
    "                            best_conf_mat = conf_mat\n",
    "                            best_f1 = f1\n",
    "print(f'Confusion matrix for ({i1,i2,i3,i4,i5}) included and {thresh}:\\n', f' {conf_mat}')\n",
    "print(f'Best f1 score: {best_f1:.2f}')\n",
    "                        #     print(f'Accuracy for ({i1,i2,i3,i4,i5}) included and {thresh}:', \n",
    "                        #           f' {acc:.2f}')\n",
    "                        #     print(f'Recall for ({i1,i2,i3,i4,i5}) included and {thresh}:', \n",
    "                        #           f' {recall:.2f}')\n",
    "                        #     print(f'Precision for ({i1,i2,i3,i4,i5}) included and {thresh}:', \n",
    "                        #           f' {prec:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid_id = 333346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yinyue/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Grid ID:  333346\n",
      "Nb components:  200\n",
      "Data directory:  ../datasets/train/data_heatmap_train.csv\n"
     ]
    }
   ],
   "source": [
    "!python3 pca_pipeline.py 333346 200 '../datasets/train/data_heatmap_train.csv'\n",
    "\n",
    "pca_df = pd.read_csv('temp/pca_df.csv')\n",
    "pca_df = pca_df.iloc[:,1:]\n",
    "pca_df_inp = pca_df.iloc[:,:-2]\n",
    "y_train = pca_df[['label']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yinyue/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Grid ID:  333346\n",
      "Nb components:  200\n",
      "Data directory:  ../datasets/test/data_heatmap_test.csv\n"
     ]
    }
   ],
   "source": [
    "!python3 pca_pipeline.py 333346 200 '../datasets/test/data_heatmap_test.csv'\n",
    "\n",
    "pca_test = pd.read_csv('temp/pca_df.csv')\n",
    "pca_test = pca_test.iloc[:,1:]\n",
    "y_test = pca_test[['label']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best PCA dimension: 54\n",
      "Best accuracy: 0.90\n",
      "Best precision: 1.00\n",
      "Best recall: 0.20\n",
      "Best f1-score: 0.34\n"
     ]
    }
   ],
   "source": [
    "# kchoices = [1,2,3,4,5,6]\n",
    "\n",
    "best_acc = -float('inf')\n",
    "best_dim = -1\n",
    "best_recall = -1\n",
    "best_prec = -1\n",
    "best_f1 = -1\n",
    "\n",
    "# for k in kchoices:\n",
    "for pca_dim in np.linspace(2,200,50).astype(int):\n",
    "      X = pca_df_inp.iloc[:,:pca_dim]\n",
    "      kmeans = KMeans(n_clusters = 2, random_state=42)\n",
    "      kmeans_labels = kmeans.fit_predict(X)\n",
    "      # Identify the smallest cluster or the one furthest from others\n",
    "      # cluster_minority = pd.Series(clusters).value_counts()\n",
    "      # cluster_minority = cluster_minority[cluster_minority <= 60].index.get_level_values(0).tolist()\n",
    "      # kmeans_labels = np.isin(clusters, cluster_minority).astype(int)\n",
    "      conf_mat   = confusion_matrix(y_train,kmeans_labels)\n",
    "      acc        = accuracy_score(y_train,kmeans_labels)\n",
    "      recall     = recall_score(y_train,kmeans_labels)\n",
    "      prec       = precision_score(y_train,kmeans_labels)\n",
    "      f1         = f1_score(y_train,kmeans_labels)\n",
    "      \n",
    "      if f1 > best_f1:\n",
    "            best_acc = acc\n",
    "            best_dim = pca_dim\n",
    "            best_recall  = recall\n",
    "            best_prec = prec\n",
    "            best_f1 = f1\n",
    "            # best_k = k\n",
    "            # print(f'Confusion matrix for {pca_dim} PCA dimensions:\\n', \n",
    "            #       f' {conf_mat}')\n",
    "            # print(f'Accuracy for {pca_dim} PCA dimensions:', \n",
    "            #       f' {acc:.2f}')\n",
    "            # print(f'Recall for {pca_dim} PCA dimensions:', \n",
    "            #       f' {recall:.2f}')\n",
    "            # print(f'Precision for {pca_dim} PCA dimensions:', \n",
    "            #       f' {prec:.2f}')\n",
    "            # print(f'---------')\n",
    "#     print(f'-------------------------------------')\n",
    "# print(f'Best k: {best_k}')\n",
    "\n",
    "print(f'Best PCA dimension: {best_dim}')\n",
    "print(f'Best accuracy: {best_acc:.2f}')\n",
    "print(f'Best precision: {best_prec:.2f}')\n",
    "print(f'Best recall: {best_recall:.2f}')\n",
    "print(f'Best f1-score: {best_f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[225   0]\n",
      " [ 31   3]]\n",
      "Test accuracy: 0.88\n",
      "Test precision: 1.00\n",
      "Test recall: 0.09\n",
      "Test f1-score: 0.16\n"
     ]
    }
   ],
   "source": [
    "X = pca_df_inp.iloc[:,:best_dim]\n",
    "kmeans = KMeans(n_clusters = 2, random_state=42)\n",
    "kmeans.fit(X)\n",
    "# clusters = pd.DataFrame(kmeans.labels_)\n",
    "# Identify the smallest cluster or the one furthest from others\n",
    "# cluster_minority = clusters.value_counts()\n",
    "# cluster_minority = cluster_minority[cluster_minority <= 60].index.get_level_values(0).tolist()\n",
    "kmeans_labels = kmeans.predict(pca_test.iloc[:,:best_dim])\n",
    "# kmeans_labels = np.isin(pred, cluster_minority).astype(int)\n",
    "conf_mat = confusion_matrix(y_test,kmeans_labels)\n",
    "acc = accuracy_score(y_test,kmeans_labels)\n",
    "recall = recall_score(y_test,kmeans_labels)\n",
    "prec = precision_score(y_test,kmeans_labels)\n",
    "f1 = f1_score(y_test,kmeans_labels)\n",
    "print(conf_mat)\n",
    "print(f'Test accuracy: {acc:.2f}')\n",
    "print(f'Test precision: {prec:.2f}')\n",
    "print(f'Test recall: {recall:.2f}')\n",
    "print(f'Test f1-score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix on X_test:\n",
      "[[224   1]\n",
      " [  9  25]]\n",
      "Accuracy on X_test: 0.96\n",
      "F1 Score on X_test: 0.83\n",
      "Predicted knn labels:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Perform PCA on X_train and X_test with 6 dimensions\n",
    "X_train = pca_df_inp.iloc[:, :6]  \n",
    "X_test = pca_test.iloc[:, :6] \n",
    "\n",
    "# Train k-NN model on X_train_pca with 10 neighbors\n",
    "knn = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "knn.fit(X_train)\n",
    "\n",
    "# Predict anomalies on X_test_pca\n",
    "distances, indices = knn.kneighbors(X_test)\n",
    "anomaly_scores = distances.mean(axis=1)\n",
    "threshold = np.percentile(anomaly_scores, 90)\n",
    "\n",
    "# Convert labels to binary (0: inliers, 1: outliers)\n",
    "knn_labels = np.where(anomaly_scores > threshold, 1, 0)\n",
    "\n",
    "# Evaluate the performance on X_test\n",
    "conf_mat = confusion_matrix(y_test, knn_labels)\n",
    "acc = accuracy_score(y_test, knn_labels)\n",
    "f1 = f1_score(y_test, knn_labels)\n",
    "\n",
    "print(f'Confusion matrix on X_test:\\n{conf_mat}')\n",
    "print(f'Accuracy on X_test: {acc:.2f}')\n",
    "print(f'F1 Score on X_test: {f1:.2f}')\n",
    "print(\"Predicted knn labels:\")\n",
    "print(knn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance on test data:\n",
      "Confusion matrix:\n",
      "[[213  12]\n",
      " [  0  34]]\n",
      "Accuracy: 0.9536679536679536\n",
      "F1 Score: 0.85\n",
      "Predicted DBScan labels:\n",
      "[0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA with 12 dimensions\n",
    "X_train = pca_df_inp.iloc[:, :12]  \n",
    "X_test = pca_test.iloc[:, :12] \n",
    "\n",
    "# Fit DBSCAN model\n",
    "dbscan = DBSCAN(eps=10, min_samples=10)\n",
    "dbscan_labels_train = dbscan.fit_predict(X_train)\n",
    "dbscan_labels_test = dbscan.fit_predict(X_test)\n",
    "\n",
    "# Convert labels to binary (0: inliers, 1: outliers)\n",
    "dbscan_labels_train = np.where(dbscan_labels_train >= 0, 0, 1)\n",
    "dbscan_labels_test = np.where(dbscan_labels_test >= 0, 0, 1)\n",
    "\n",
    "# Evaluate performance on test data\n",
    "conf_mat_test = confusion_matrix(y_test, dbscan_labels_test)\n",
    "acc_test = accuracy_score(y_test, dbscan_labels_test)\n",
    "f1_test = f1_score(y_test, dbscan_labels_test)\n",
    "\n",
    "print(\"\\nPerformance on test data:\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_mat_test)\n",
    "print(\"Accuracy:\", acc_test)\n",
    "print(\"F1 Score:\", f1_test)\n",
    "print(\"Predicted DBScan labels:\")\n",
    "print(dbscan_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for 6 PCA dimensions:\n",
      "  [[223   2]\n",
      " [  1  33]]\n",
      "F1 Score for 6 PCA dimensions with n_estimator = 50:  0.96\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "#Best Model for Isolation Forest is with 10 PCA and n_estimators = 100\n",
    "best_n_est = 50\n",
    "pca_dim = 6\n",
    "X_train = pca_df_inp.iloc[:,:pca_dim]\n",
    "X_test = pca_test.iloc[:,:pca_dim]\n",
    "            \n",
    "isolation_forest = IsolationForest(n_estimators=best_n_est, \n",
    "                                   max_samples='auto', \n",
    "                                   random_state=42)\n",
    "\n",
    "model = isolation_forest.fit(X_train)\n",
    "if_labels = model.predict(X_test)\n",
    "if_labels = np.where(if_labels == -1, 1, 0)\n",
    "\n",
    "conf_mat   = confusion_matrix(y_test,if_labels)\n",
    "acc        = accuracy_score(y_test,if_labels)\n",
    "recall     = recall_score(y_test,if_labels)\n",
    "prec       = precision_score(y_test,if_labels)\n",
    "f1         = f1_score(y_test, if_labels)\n",
    "\n",
    "\n",
    "print(f'Confusion matrix for {pca_dim} PCA dimensions:\\n', \n",
    "                      f' {conf_mat}')\n",
    "#print(f'Accuracy for {pca_dim} PCA dimensions:', \n",
    "#                      f' {acc:.2f}')\n",
    "#print(f'Recall for {pca_dim} PCA dimensions:', \n",
    "#                      f' {recall:.2f}')\n",
    "#print(f'Precision for {pca_dim} PCA dimensions:', \n",
    "#                      f' {prec:.2f}')\n",
    "print(f'F1 Score for {pca_dim} PCA dimensions with n_estimator = {best_n_est}:', \n",
    "                      f' {f1:.2f}')\n",
    "print(f'---------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for 85 PCA dimensions:\n",
      "  [[ 69 156]\n",
      " [  0  34]]\n",
      "F1 Score for 85 PCA dimensions:  0.30\n"
     ]
    }
   ],
   "source": [
    "# Best Model is with 163 PCA dimensions\n",
    "pca_dim = 85\n",
    "\n",
    "X_train = pca_df_inp.iloc[:,:pca_dim]\n",
    "X_test = pca_test.iloc[:,:pca_dim]\n",
    "\n",
    "oneclass = OneClassSVM(gamma = 'auto').fit(X_train)\n",
    "oneclass_labels = oneclass.predict(X_test)\n",
    "    \n",
    "oneclass_labels = np.where(oneclass_labels == 1, 0,1)\n",
    "conf_mat   = confusion_matrix(y_test,oneclass_labels)\n",
    "acc        = accuracy_score(y_test,oneclass_labels)\n",
    "recall     = recall_score(y_test,oneclass_labels)\n",
    "prec       = precision_score(y_test,oneclass_labels)\n",
    "f1         = f1_score(y_test,oneclass_labels)\n",
    "\n",
    "print(f'Confusion matrix for {pca_dim} PCA dimensions:\\n', \n",
    "              f' {conf_mat}')\n",
    "#print(f'Accuracy for {pca_dim} PCA dimensions:', \n",
    "#              f' {acc:.2f}')\n",
    "#print(f'Recall for {pca_dim} PCA dimensions:', \n",
    "#              f' {recall:.2f}')\n",
    "#print(f'Precision for {pca_dim} PCA dimensions:', \n",
    "#              f' {prec:.2f}')\n",
    "print(f'F1 Score for {pca_dim} PCA dimensions:', \n",
    "              f' {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for ((1, 1, 1, 1, 1)) included and 1:\n",
      "  [[224   1]\n",
      " [ 10  24]]\n",
      "Best f1 score: 0.97\n"
     ]
    }
   ],
   "source": [
    "best_acc = -float('inf')\n",
    "best_dim = -1\n",
    "best_recall = -1\n",
    "best_prec = -1\n",
    "best_f1 = -1\n",
    "\n",
    "for i1 in [0,1]:\n",
    "    for i2 in [0,1]:\n",
    "        for i3 in [0,1]:\n",
    "            for i4 in [0,1]:\n",
    "                for i5 in [0,1]:\n",
    "                    for thresh in [0,1]:\n",
    "                        tot = i1+i2+i3+i4+i5\n",
    "                        comb_labels = i1*if_labels + i2*oneclass_labels + i3*dbscan_labels_test + \\\n",
    "                                        i4*kmeans_labels + i5*knn_labels\n",
    "                        comb_labels = np.where(comb_labels > tot//2 + thresh, 1, 0)\n",
    "\n",
    "                        conf_mat   = confusion_matrix(y_test,comb_labels)\n",
    "                        acc        = accuracy_score(y_test,comb_labels)\n",
    "                        recall     = recall_score(y_test,comb_labels)\n",
    "                        prec       = precision_score(y_test,comb_labels)\n",
    "                        f1         = f1_score(y_test,comb_labels)\n",
    "\n",
    "                        if acc > best_acc:\n",
    "                            best_acc = acc\n",
    "                            best_recall  = recall\n",
    "                            best_prec = prec\n",
    "                            best_conf_mat = conf_mat\n",
    "                            best_f1 = f1\n",
    "print(f'Confusion matrix for ({i1,i2,i3,i4,i5}) included and {thresh}:\\n', f' {conf_mat}')\n",
    "print(f'Best f1 score: {best_f1:.2f}')\n",
    "                        #     print(f'Accuracy for ({i1,i2,i3,i4,i5}) included and {thresh}:', \n",
    "                        #           f' {acc:.2f}')\n",
    "                        #     print(f'Recall for ({i1,i2,i3,i4,i5}) included and {thresh}:', \n",
    "                        #           f' {recall:.2f}')\n",
    "                        #     print(f'Precision for ({i1,i2,i3,i4,i5}) included and {thresh}:', \n",
    "                        #           f' {prec:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
